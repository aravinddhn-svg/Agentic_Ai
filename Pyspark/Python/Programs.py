case1:

You are given a dataset of user clickstream data containing user_id, timestamp, 
and page_id. Write a Spark code snippet to calculate the average session duration for each user, w
here a session is defined as a sequence of clicks by the same user within a 30-minute window of inactivity.


Use case --> Need to calculate the user average session,

Note if the user croseed the 30 minutes window then new session id got created.a


Solution :

We need to cal to gap to identify the session.a

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from datetime import datetime

spark = SparkSession.builder.getOrCreate()

data = [
    ("u1", "2024-01-01 10:00:00", "A"),
    ("u1", "2024-01-01 10:10:00", "B"),
    ("u1", "2024-01-01 10:12:00", "C"),
    ("u1", "2024-01-01 13:00:00", "D"),   # gap > 30min → new session
    ("u1", "2024-01-01 13:10:00", "E"),
    ("u1", "2024-01-12 13:10:00", "F"),

    ("u2", "2024-01-01 09:00:00", "X"),
    ("u2", "2024-01-01 09:45:00", "Y"),   # gap > 30min → new session
    ("u2", "2024-01-01 10:10:00", "Z"),   # gap < 30min → same session?
]

schema = "user_id string, ts string, page_id string"
df = spark.createDataFrame(data, schema)



df=df.withColumn("ts1",unix_timestamp("ts")).withColumn("ts2",col("ts").cast("timestamp"))
>>> df.show()


+-------+-------------------+-------+----------+-------------------+
|user_id|                 ts|page_id|       ts1|                ts2|
+-------+-------------------+-------+----------+-------------------+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|
+-------+-------------------+-------+----------+-------------------+

from pyspark.sql.window import Window


To calcuate the time differnece

step1:

If the ts in string conevrt to unix_timestamp (into seconds) then just take differemc

step2:

if the column already in timestamp while then convert to long and take diff


from pyspark.sql.window import Window


w1=Window.partitionBy("user_id").orderBy("ts1")

w2=Window.partitionBy("user_id").orderBy("ts2")


df2=df.withColumn("prev_ts1",lag("ts1").over(w1)).withColumn("prev_ts2",lag("ts2").over(w2))

df2=df.withColumn("prev_ts1",lag("ts1").over(w1)).withColumn("prev_ts2",lag("ts2").over(w2))
>>> 
>>> 
>>> df2.show()
+-------+-------------------+-------+----------+-------------------+----------+-------------------+
|user_id|                 ts|page_id|       ts1|                ts2|  prev_ts1|           prev_ts2|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|      NULL|               NULL|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|1704083400|2024-01-01 10:00:00|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|1704084000|2024-01-01 10:10:00|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|1704084120|2024-01-01 10:12:00|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|1704094200|2024-01-01 13:00:00|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|      NULL|               NULL|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|1704079800|2024-01-01 09:00:00|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|1704082500|2024-01-01 09:45:00|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+

df3=df2.withColumn("gap1",col("ts1")-col("prev_ts1"))

df3=df2.withColumn("gap1",col("ts1")-col("prev_ts1"))
>>> 
>>> df3.show()
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+
|user_id|                 ts|page_id|       ts1|                ts2|  prev_ts1|           prev_ts2| gap1|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|      NULL|               NULL| NULL|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|1704083400|2024-01-01 10:00:00|  600|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|1704084000|2024-01-01 10:10:00|  120|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|1704084120|2024-01-01 10:12:00|10080|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|1704094200|2024-01-01 13:00:00|  600|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|      NULL|               NULL| NULL|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|1704079800|2024-01-01 09:00:00| 2700|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|1704082500|2024-01-01 09:45:00| 1500|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+

>>> df3=df3.withColumn("gap2",col("ts2").cast("long")-col("prev_ts2").cast("long"))
>>> df3.show()
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+-----+
|user_id|                 ts|page_id|       ts1|                ts2|  prev_ts1|           prev_ts2| gap1| gap2|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+-----+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|      NULL|               NULL| NULL| NULL|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|1704083400|2024-01-01 10:00:00|  600|  600|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|1704084000|2024-01-01 10:10:00|  120|  120|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|1704084120|2024-01-01 10:12:00|10080|10080|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|1704094200|2024-01-01 13:00:00|  600|  600|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|      NULL|               NULL| NULL| NULL|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|1704079800|2024-01-01 09:00:00| 2700| 2700|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|1704082500|2024-01-01 09:45:00| 1500| 1500|
+-------+-------------------+-------+----------+-------------------+----------+-------------------+-----+-----+


step 4: Cal the session crossed the 30 minutes



df4=df3.withColumn("sess",when(col("gap1").isNull() | (col("gap1")>30*60) ,1).otherwise(0)).drop("gap2","prev_ts2")


 df4.show()

+-------+-------------------+-------+----------+-------------------+----------+------+----+
|user_id|                 ts|page_id|       ts1|                ts2|  prev_ts1|  gap1|sess|
+-------+-------------------+-------+----------+-------------------+----------+------+----+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|      NULL|  NULL|   1|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|1704083400|   600|   0|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|1704084000|   120|   0|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|1704084120| 10080|   1|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|1704094200|   600|   0|
|     u1|2024-01-12 13:10:00|      F|1705045200|2024-01-12 13:10:00|1704094800|950400|   1|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|      NULL|  NULL|   1|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|1704079800|  2700|   1|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|1704082500|  1500|   0|
+-------+-------------------+-------+----------+-------------------+----------+------+----+


Always assume first records is new ssseion id so 1 by default:

df4.show()


now calculate the session id using cummulative sum:

w11=Window.partitionBy("user_id").orderBy("ts1")

df5=df4.withColumn("nw_s",sum("sess").over(w11))


df5.show()

+-------+-------------------+-------+----------+-------------------+----------+------+----+----+
|user_id|                 ts|page_id|       ts1|                ts2|  prev_ts1|  gap1|sess|nw_s|
+-------+-------------------+-------+----------+-------------------+----------+------+----+----+
|     u1|2024-01-01 10:00:00|      A|1704083400|2024-01-01 10:00:00|      NULL|  NULL|   1|   1|
|     u1|2024-01-01 10:10:00|      B|1704084000|2024-01-01 10:10:00|1704083400|   600|   0|   1|
|     u1|2024-01-01 10:12:00|      C|1704084120|2024-01-01 10:12:00|1704084000|   120|   0|   1|
|     u1|2024-01-01 13:00:00|      D|1704094200|2024-01-01 13:00:00|1704084120| 10080|   1|   2|
|     u1|2024-01-01 13:10:00|      E|1704094800|2024-01-01 13:10:00|1704094200|   600|   0|   2|
|     u1|2024-01-12 13:10:00|      F|1705045200|2024-01-12 13:10:00|1704094800|950400|   1|   3|
|     u2|2024-01-01 09:00:00|      X|1704079800|2024-01-01 09:00:00|      NULL|  NULL|   1|   1|
|     u2|2024-01-01 09:45:00|      Y|1704082500|2024-01-01 09:45:00|1704079800|  2700|   1|   2|
|     u2|2024-01-01 10:10:00|      Z|1704084000|2024-01-01 10:10:00|1704082500|  1500|   0|   2|
+-------+-------------------+-------+----------+-------------------+----------+------+----+----+

--- cal abg time per seesion

2 steps:

1: calculate the diff min and max per seesion

df6 = df5.groupBy("user_id","nw_s").agg(max("ts").alias("mx"),min("ts").alias("mn")).withColumn("diffpersess",((to_unix_timestamp("mx")-to_unix_timestamp("mn"))/60).cast("int"))
>>> df6.show()
+-------+----+-------------------+-------------------+-----------+
|user_id|nw_s|                 mx|                 mn|diffpersess|
+-------+----+-------------------+-------------------+-----------+
|     u1|   1|2024-01-01 10:12:00|2024-01-01 10:00:00|         12|
|     u1|   2|2024-01-01 13:10:00|2024-01-01 13:00:00|         10|
|     u1|   3|2024-01-12 13:10:00|2024-01-12 13:10:00|          0|
|     u2|   1|2024-01-01 09:00:00|2024-01-01 09:00:00|          0|
|     u2|   2|2024-01-01 10:10:00|2024-01-01 09:45:00|         25|
+-------+----+-------------------+-------------------+-----------+


df6 = df5.groupBy("user_id","nw_s").agg(max("ts").alias("mx"),min("ts").alias("mn")).withColumn("diffpersess",((to_unix_timestamp("mx")-to_unix_timestamp("mn"))/60).cast("int"))

df7= df6.groupBy("user_id","nw_s").agg(avg("diffpersess").alias("avg_s").cast("int"))


 df7.show()
+-------+----+-----+
|user_id|nw_s|avg_s|
+-------+----+-----+
|     u1|   1|   12|
|     u1|   2|   10|
|     u1|   3|    0|
|     u2|   1|    0|
|     u2|   2|   25|



park = SparkSession.builder.getOrCreate()

data = [
    ("u1", "2024-01-01 10:00:00", "A"),
    ("u1", "2024-01-01 10:10:00", "B"),
    ("u1", "2024-01-01 10:12:00", "C"),
    ("u1", "2024-01-01 13:00:00", "D"),   # gap > 30min → new session
    ("u1", "2024-01-01 13:10:00", "E"),
    ("u1", "2024-01-12 13:10:00", "F"),

    ("u2", "2024-01-01 09:00:00", "X"),
    ("u2", "2024-01-01 09:45:00", "Y"),   # gap > 30min → new session
    ("u2", "2024-01-01 10:10:00", "Z"),   # gap < 30min → same session?
]

schema = "user_id string, ts string, page_id string"
df = spark.createDataFrame(data, schema)



df=df.withColumn("ts1",unix_timestamp("ts")).withColumn("ts2",col("ts").cast("timestamp"))


from pyspark.sql.window import Window


w1=Window.partitionBy("user_id").orderBy("ts1")

w2=Window.partitionBy("user_id").orderBy("ts2")


df2=df.withColumn("prev_ts1",lag("ts1").over(w1)).withColumn("prev_ts2",lag("ts2").over(w2))

df3=df2.withColumn("gap1",col("ts1")-col("prev_ts1"))

df4=df3.withColumn("sess",when(col("gap1").isNull() | (col("gap1")>30*60) ,1).otherwise(0)).drop("gap2","prev_ts2")

w11=Window.partitionBy("user_id").orderBy("ts1")

df5=df4.withColumn("nw_s",sum("sess").over(w11))
df6 = df5.groupBy("user_id","nw_s").agg(max("ts").alias("mx"),min("ts").alias("mn")).withColumn("diffpersess",((to_unix_timestamp("mx")-to_unix_timestamp("mn"))/60).cast("int"))


###correct one need to calculate the average duration per user.
    df_avg = df_session.groupBy("user_id") \
    .agg(
        avg("session_duration_min").cast("int").alias("avg_session_duration")
    )


##df7= df6.groupBy("user_id","nw_s").agg(avg("diffpersess").alias("avg_s").cast("int"))




problem 2:

from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import Window

Need to flag if the user make more than 10 transaction in  24 hours. 

data = [
    ("t1", "u1", 100, "2024-01-01 09:00:00"),
    ("t2", "u1",  50, "2024-01-01 10:00:00"),
    ("t3", "u1",  20, "2024-01-01 11:00:00"),
    ("t4", "u1",  60, "2024-01-01 12:00:00"),
    ("t5", "u1",  10, "2024-01-01 13:00:00"),
    ("t6", "u1",  80, "2024-01-01 14:00:00"),
    ("t7", "u1", 200, "2024-01-01 15:00:00"),
    ("t8", "u1", 250, "2024-01-01 16:00:00"),
    ("t9", "u1", 500, "2024-01-01 17:00:00"),
    ("t10","u1",  40, "2024-01-01 18:00:00"),
    ("t11","u1",  30, "2024-01-01 18:01:00"),  # 11th trx inside 24h window → fraud
    ("t12","u2",  30, "2024-01-01 18:01:00"), 
    ("t12","u3",  30, "2024-01-01 18:01:00"),
     ("t1",  "u4", 100, "2024-01-01 09:00:00"),
    ("t2", "u4",  50, "2024-01-01 10:00:00"),
    ("t3", "u4",  20, "2024-01-01 11:00:00"),
    ("t4", "u4",  60, "2024-01-01 12:00:00"),
    ("t5", "u4",  10, "2024-01-01 13:00:00"),
    ("t6", "u4",  80, "2024-01-01 14:00:00"),
    ("t7", "u4", 200, "2024-01-01 15:00:00"),
    ("t8", "u4", 250, "2024-01-03 16:00:00"),
    ("t9", "u4", 500, "2024-01-03 17:00:00"),
    ("t10","u4",  40, "2024-01-05 18:00:00")    
          
          
]

schema = "txn_id string, user_id string, amount int, timestamp string"
txn = spark.createDataFrame(data, schema)
txn = txn.withColumn("timestamp", to_timestamp("timestamp")).withColumn("ts",to_unix_timestamp("timestamp"))
txn.show()


+------+-------+------+-------------------+----------+
|txn_id|user_id|amount|          timestamp|    ts|
+------+-------+------+-------------------+----------+
|    t1|     u1|   100|2024-01-01 09:00:00|1704079800|
|    t2|     u1|    50|2024-01-01 10:00:00|1704083400|
|    t3|     u1|    20|2024-01-01 11:00:00|1704087000|
|    t4|     u1|    60|2024-01-01 12:00:00|1704090600|
|    t5|     u1|    10|2024-01-01 13:00:00|1704094200|
|    t6|     u1|    80|2024-01-01 14:00:00|1704097800|
|    t7|     u1|   200|2024-01-01 15:00:00|1704101400|
|    t8|     u1|   250|2024-01-01 16:00:00|1704105000|
|    t9|     u1|   500|2024-01-01 17:00:00|1704108600|
|   t10|     u1|    40|2024-01-01 18:00:00|1704112200|
|   t11|     u1|    30|2024-01-01 18:01:00|1704112260|
+------+-------+------+-------------------+----------+


txn = txn.withColumn("ts", col("timestamp").cast("long"))

w=Window.partitionBy("user_id").orderBy("ts").rangeBetween(-60*60*24,0)

tx2=txn.withColumn("cnt_24h",count("*").over(w))

txn3 = tx2.withColumn(
    "fraud_flag", when(col("cnt_24h") > 10, 1).otherwise(0)
)
txn3.show()


fraud_users = txn3.filter("fraud_flag = 1").select("user_id").distinct()
fraud_users.show()







